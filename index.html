<!DOCTYPE html>
<html lang="en-us">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
  <head>
    <meta charset="UTF-8">
    <title>Convex optimization by wher0001</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Convex Optimization</h1>
      <h2 class="project-tagline">Yup, you heard me!</h2>
      <a href="https://github.com/wher0001/convex_optimization" class="btn">View on GitHub</a>
    </section>

    <section class="main-content">
    <h2> <a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome to <a href="https://github.com/wher0001" class="user-mention">@wher0001</a>'s site on Gradient Descent algorithms.</h2>
    <br />
    <p>In this instance we will be considering only the supervised learning aspect. My job provides me with hundreds of datasets to work with. Almost all of these training sets should yield good results. The questions is always whether we need to modify our learning rates or decay rates for a specific training set. I will show here that the current methods, including the latest algorithms based strictly upon mathematical principles, do not fulfill our needs and therefore there remains a problem that needs to be addressed.</p>

    <p>Gradient Descent is a first-order approximation algorithm. It is a minimization algorithm that best operates on a convex or pseudo-convex Cost Function. This cost function is typically predefined although <b>self.author</b> has found that creating a convex Cost Function can improve the results if the intended output is well-known</p>
    <h2>Batch Gradient Descent</h2>
    In general we have some set of equations that we are trying to solve. The Standard way to solve for this is by using Iterative Least-Squares Cost function technique. This technique involves having a Cost function $J(\mathbf{\theta})$ that makes the problem convex. It is generally practiced that the $$J(\mathbf{\theta}) = \frac{1}{2}\sum_{i=1}^n (h_\theta( \mathbf{x^{(i)}} ) - y^{(i)})^2$$ where $$h_\theta(\mathbf{x^{(i)}}) = \theta^\intercal x$$
    <h2>Mini-Batch Gradient Descent w/ Randomized Mini-Batch</h2>
    
    <h2>Stochastic Gradient Descent</h2>
    <h2>Mini-Batch SGD w/ Randomized Mini-Batch</h2>
    <h2>SGD with Momentum</h2>
    <h2>Nesterov Momentum SGD</h2>
    <h2>Averaged SGD</h2>
    <h2>AdaGrad</h2>
    <h2>RMSProp</h2>
    <h2>Adam</h2>
    <p>'Adam' (for Adaptive Moment Estimation) is an update to ''RMSProp'' optimizer. In this running average of both the gradients and their magnitudes are used. The three equations that define this optimizer is as follows,
    
    $$m(w,t):=\beta_1 * m(w,t-1) + (1 - \beta_1) * \nabla Q_i(w)$$
    $$v(w,t):=\beta_2 * v(w,t-1) + (1 - \beta_2) * (\nabla Q_i(w))^2 $$
    
    
    $$\hat{m}(w,t):=\frac{m(w,t)}{(1 - \beta_1^t)}$$
    $$\hat{v}(w,t):=\frac{v(w,t)}{(1 - \beta_2^t)}$$
    
    
    $$w:=w-\frac{\eta}{\sqrt{\hat{v}(w,t)} + \epsilon} * \hat{m}(w,t)$$
    
    where, $\beta_1 , \beta_2$ are two forgetting factors of the algorithm, respectively for gradients and magnitude of gradients.</p>
    
    <h3>
    <a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Designer Templates</h3>
    
    <p>We’ve crafted some handsome templates for you to use. Go ahead and click 'Continue to layouts' to browse through them. You can easily go back to edit your page before publishing. After publishing your page, you can revisit the page generator and switch to another theme. Your Page content will be preserved.</p>
    
    <h3>
    <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating pages manually</h3>
    
    <p>If you prefer to not use the automatic generator, push a branch named <code>gh-pages</code> to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.</p>
    
    <h3>
    <a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>
    
    <p>You can send me (<a href="https://github.com/wher0001" class="user-mention">@wher0001</a>) a private message if you like until I get the comments section working. (I may not even try).</p>
    
    <h3>
    <a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>
    
    <p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>
    
    <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/wher0001/convex_optimization">Convex optimization</a> is maintained by <a href="https://github.com/wher0001">wher0001</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
